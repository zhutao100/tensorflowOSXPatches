diff --git a/build-mac-os-x.sh b/build-mac-os-x.sh
new file mode 100755
index 0000000000..e52fee3f7f
--- /dev/null
+++ b/build-mac-os-x.sh
@@ -0,0 +1,20 @@
+#!/bin/bash
+
+# Config
+./configure
+
+# Build
+bazel build --config=opt --action_env PATH --action_env LD_LIBRARY_PATH --action_env DYLD_LIBRARY_PATH --action_env TMP //tensorflow/tools/pip_package:build_pip_package
+
+# Replace _nccl_ops.so
+gcc -march=native -c -fPIC tensorflow/contrib/nccl/kernels/nccl_ops.cc -o _nccl_ops.o
+gcc _nccl_ops.o -shared -o _nccl_ops.so
+rm _nccl_ops.o
+if [ -d "./bazel-out/darwin-opt" ]; then
+  mv _nccl_ops.so bazel-out/darwin-opt/bin/tensorflow/contrib/nccl/python/ops
+elif [ -d "./bazel-out/darwin-py3-opt" ]; then
+  mv _nccl_ops.so bazel-out/darwin-py3-opt/bin/tensorflow/contrib/nccl/python/ops
+fi
+
+# Generate package
+bazel-bin/tensorflow/tools/pip_package/build_pip_package .
diff --git a/.gitignore b/.gitignore
index be75938ec4..62b745e368 100644
--- a/.gitignore
+++ b/.gitignore
@@ -27,6 +27,7 @@ Podfile.lock
 /tensorflow/contrib/lite/examples/ios/simple/data/*.txt
 /tensorflow/contrib/lite/examples/ios/simple/data/*.tflite
 xcuserdata/**
+*.whl
 
 # Android
 .gradle
diff --git a/tensorflow/core/framework/variant.h b/tensorflow/core/framework/variant.h
index c02391dae3..23e56ed294 100644
--- a/tensorflow/core/framework/variant.h
+++ b/tensorflow/core/framework/variant.h
@@ -152,7 +152,8 @@ bool DecodeVariant(const string& buf, T* value);
 //
 class Variant {
  public:
-  constexpr Variant() noexcept = default;
+  //constexpr Variant() noexcept = default;
+  Variant() noexcept = default;
 
   Variant(const Variant& other)
       : value_(other.is_empty() ? std::unique_ptr<ValueInterface>()
diff --git a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc
index 0f7adaf24a..8d89c66f3f 100644
--- a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc
+++ b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc
@@ -69,7 +69,7 @@ __global__ void concat_variable_kernel(
   IntType num_inputs = input_ptr_data.size;
 
   // verbose declaration needed due to template
-  extern __shared__ __align__(sizeof(T)) unsigned char smem[];
+  extern __shared__ unsigned char smem[];
   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);
 
   if (useSmem) {
diff --git a/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc b/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc
index 94989089ec..a2e3e8bc87 100644
--- a/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc
+++ b/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc
@@ -172,7 +172,7 @@ __global__ __launch_bounds__(1024, 2) void DepthwiseConv2dGPUKernelNHWCSmall(
     const DepthwiseArgs args, const T* input, const T* filter, T* output) {
   assert(CanLaunchDepthwiseConv2dGPUSmall(args));
   // Holds block plus halo and filter data for blockDim.x depths.
-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];
+  extern __shared__ unsigned char shared_memory[];
   T* const shared_data = reinterpret_cast<T*>(shared_memory);
 
   const int num_batches = args.batch;
@@ -452,7 +452,7 @@ __global__ __launch_bounds__(1024, 2) void DepthwiseConv2dGPUKernelNCHWSmall(
     const DepthwiseArgs args, const T* input, const T* filter, T* output) {
   assert(CanLaunchDepthwiseConv2dGPUSmall(args));
   // Holds block plus halo and filter data for blockDim.z depths.
-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];
+  extern __shared__ unsigned char shared_memory[];
   T* const shared_data = reinterpret_cast<T*>(shared_memory);
 
   const int num_batches = args.batch;
@@ -1118,7 +1118,7 @@ __launch_bounds__(1024, 2) void DepthwiseConv2dBackpropFilterGPUKernelNHWCSmall(
     const DepthwiseArgs args, const T* output, const T* input, T* filter) {
   assert(CanLaunchDepthwiseConv2dBackpropFilterGPUSmall(args, blockDim.z));
   // Holds block plus halo and filter data for blockDim.x depths.
-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];
+  extern __shared__ unsigned char shared_memory[];
   T* const shared_data = reinterpret_cast<T*>(shared_memory);
 
   const int num_batches = args.batch;
@@ -1388,7 +1388,7 @@ __launch_bounds__(1024, 2) void DepthwiseConv2dBackpropFilterGPUKernelNCHWSmall(
     const DepthwiseArgs args, const T* output, const T* input, T* filter) {
   assert(CanLaunchDepthwiseConv2dBackpropFilterGPUSmall(args, blockDim.x));
   // Holds block plus halo and filter data for blockDim.z depths.
-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];
+  extern __shared__ unsigned char shared_memory[];
   T* const shared_data = reinterpret_cast<T*>(shared_memory);
 
   const int num_batches = args.batch;
diff --git a/tensorflow/core/kernels/split_lib_gpu.cu.cc b/tensorflow/core/kernels/split_lib_gpu.cu.cc
index 393818730b..a7d9e02853 100644
--- a/tensorflow/core/kernels/split_lib_gpu.cu.cc
+++ b/tensorflow/core/kernels/split_lib_gpu.cu.cc
@@ -121,7 +121,7 @@ __global__ void split_v_kernel(const T* input_ptr,
   int num_outputs = output_ptr_data.size;
 
   // verbose declaration needed due to template
-  extern __shared__ __align__(sizeof(T)) unsigned char smem[];
+  extern __shared__ unsigned char smem[];
   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);
 
   if (useSmem) {
diff --git a/tensorflow/workspace.bzl b/tensorflow/workspace.bzl
index e9e69203db..0f1ce8e010 100644
--- a/tensorflow/workspace.bzl
+++ b/tensorflow/workspace.bzl
@@ -127,11 +127,11 @@ def tf_workspace(path_prefix="", tf_repo_name=""):
   tf_http_archive(
       name = "eigen_archive",
       urls = [
-          "https://mirror.bazel.build/bitbucket.org/eigen/eigen/get/2355b229ea4c.tar.gz",
-          "https://bitbucket.org/eigen/eigen/get/2355b229ea4c.tar.gz",
+          "https://mirror.bazel.build/bitbucket.org/dtrebbien/eigen/get/374842a18727.tar.gz",
+          "https://bitbucket.org/dtrebbien/eigen/get/374842a18727.tar.gz",
       ],
-      sha256 = "0cadb31a35b514bf2dfd6b5d38205da94ef326ec6908fc3fd7c269948467214f",
-      strip_prefix = "eigen-eigen-2355b229ea4c",
+      sha256 = "fa26e9b9ff3a2692b092d154685ec88d6cb84d4e1e895006541aff8603f15c16",
+      strip_prefix = "dtrebbien-eigen-374842a18727",
       build_file = str(Label("//third_party:eigen.BUILD")),
       patch_file = str(Label("//third_party:eigen_fix_cuda_compilation.patch"))
   )
@@ -361,11 +361,11 @@ def tf_workspace(path_prefix="", tf_repo_name=""):
   tf_http_archive(
       name = "protobuf_archive",
       urls = [
-          "https://mirror.bazel.build/github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz",
-          "https://github.com/google/protobuf/archive/396336eb961b75f03b25824fe86cf6490fb75e3a.tar.gz",
+          "https://mirror.bazel.build/github.com/dinever/protobuf/archive/188578878eff18c2148baba0e116d87ce8f49410.tar.gz",
+          "https://github.com/dinever/protobuf/archive/188578878eff18c2148baba0e116d87ce8f49410.tar.gz",
       ],
-      sha256 = "846d907acf472ae233ec0882ef3a2d24edbbe834b80c305e867ac65a1f2c59e3",
-      strip_prefix = "protobuf-396336eb961b75f03b25824fe86cf6490fb75e3a",
+      sha256 = "7a1d96ccdf7131535828cad737a76fd65ed766e9511e468d0daa3cc4f3db5175",
+      strip_prefix = "protobuf-188578878eff18c2148baba0e116d87ce8f49410",
   )
 
   # We need to import the protobuf library under the names com_google_protobuf
@@ -556,11 +556,11 @@ def tf_workspace(path_prefix="", tf_repo_name=""):
   tf_http_archive(
       name = "nccl_archive",
       urls = [
-          "https://mirror.bazel.build/github.com/nvidia/nccl/archive/03d856977ecbaac87e598c0c4bafca96761b9ac7.tar.gz",
-          "https://github.com/nvidia/nccl/archive/03d856977ecbaac87e598c0c4bafca96761b9ac7.tar.gz",
+          "https://mirror.bazel.build/github.com/zhutao100/nccl/archive/b2ab0f23b11db13605abe71f193650d4f6273c6b.tar.gz",
+          "https://github.com/zhutao100/nccl/archive/b2ab0f23b11db13605abe71f193650d4f6273c6b.tar.gz",
       ],
-      sha256 = "2ca86fb6179ecbff789cc67c836139c1bbc0324ed8c04643405a30bf26325176",
-      strip_prefix = "nccl-03d856977ecbaac87e598c0c4bafca96761b9ac7",
+      sha256 = "405fb578e8aaccf458e4a66baea71fe1c839b06d2983f9a03496de10f3ac1e77",
+      strip_prefix = "nccl-b2ab0f23b11db13605abe71f193650d4f6273c6b",
       build_file = str(Label("//third_party:nccl.BUILD")),
   )
 
diff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl
index 2a37c65bc7..61b203e005 100644
--- a/third_party/gpus/cuda/BUILD.tpl
+++ b/third_party/gpus/cuda/BUILD.tpl
@@ -110,7 +110,7 @@ cc_library(
         ".",
         "cuda/include",
     ],
-    linkopts = ["-lgomp"],
+    # linkopts = ["-lgomp"],
     linkstatic = 1,
     visibility = ["//visibility:public"],
 )
diff --git a/third_party/nccl/nccl.h b/third_party/nccl/nccl.h
new file mode 100644
index 0000000000..7bb5aa52bc
--- /dev/null
+++ b/third_party/nccl/nccl.h
@@ -0,0 +1,203 @@
+/*************************************************************************
+ * Copyright (c) 2015-2016, NVIDIA CORPORATION. All rights reserved.
+ *
+ * See LICENSE.txt for license information
+ ************************************************************************/
+
+#ifndef NCCL_H_
+#define NCCL_H_
+
+#include <cuda_runtime.h>
+
+#if CUDART_VERSION >= 7050
+#include <cuda_fp16.h>
+#define CUDA_HAS_HALF 1
+#else
+#undef CUDA_HAS_HALF
+#endif
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/* Opaque handle to communicator */
+typedef struct ncclComm* ncclComm_t;
+
+#define NCCL_UNIQUE_ID_BYTES 128
+typedef struct { char internal[NCCL_UNIQUE_ID_BYTES]; } ncclUniqueId;
+
+/* Error type */
+typedef enum { ncclSuccess                 =  0,
+               ncclUnhandledCudaError      =  1,
+               ncclSystemError             =  2,
+               ncclInternalError           =  3,
+               ncclInvalidDevicePointer    =  4,
+               ncclInvalidRank             =  5,
+               ncclUnsupportedDeviceCount  =  6,
+               ncclDeviceNotFound          =  7,
+               ncclInvalidDeviceIndex      =  8,
+               ncclLibWrapperNotSet        =  9,
+               ncclCudaMallocFailed        = 10,
+               ncclRankMismatch            = 11,
+               ncclInvalidArgument         = 12,
+               ncclInvalidType             = 13,
+               ncclInvalidOperation        = 14,
+               nccl_NUM_RESULTS            = 15 } ncclResult_t;
+
+/* Generates a unique Id with each call. Used to generate commId for
+ * ncclCommInitAll. uniqueId will be created in such a way that it is
+ * guaranteed to be unique accross the host. */
+ncclResult_t  ncclGetUniqueId(ncclUniqueId* uniqueId);
+ncclResult_t pncclGetUniqueId(ncclUniqueId* uniqueId);
+
+/* Creates a new communicator (multi process version).
+ * rank must be between 0 and ndev-1 and unique within a communicator clique.
+ * ndev is number of logical devices
+ * The communicator is created on the current CUDA device.
+ * ncclCommInitRank implicitly syncronizes with other ranks, so INIT OF EACH RANK MUST
+ * BE CALLED IN A SEPARATE HOST THREADS to avoid deadlock. */
+ncclResult_t  ncclCommInitRank(ncclComm_t* comm, int ndev, ncclUniqueId commId, int rank);
+ncclResult_t pncclCommInitRank(ncclComm_t* comm, int ndev, ncclUniqueId commId, int rank);
+
+/* Creates a clique of communicators.
+ * This is a convenience function to create a single-process communicator clique.
+ * Returns an array of ndev newly initialized communicators in comm.
+ * comm should be pre-allocated with size at least ndev*sizeof(ncclComm_t).
+ * If devlist is NULL, the first ndev CUDA devices are used.
+ * Order of devlist defines user-order of processors within the communicator. */
+ncclResult_t  ncclCommInitAll(ncclComm_t* comm, int ndev, const int* devlist);
+ncclResult_t pncclCommInitAll(ncclComm_t* comm, int ndev, const int* devlist);
+
+/* Frees resources associated with communicator object. */
+void  ncclCommDestroy(ncclComm_t comm);
+void pncclCommDestroy(ncclComm_t comm);
+
+/* Returns nice error message. */
+const char*  ncclGetErrorString(ncclResult_t result);
+const char* pncclGetErrorString(ncclResult_t result);
+
+/* Sets count to number of devices in the communicator clique. */
+ncclResult_t  ncclCommCount(const ncclComm_t comm, int* count);
+ncclResult_t pncclCommCount(const ncclComm_t comm, int* count);
+
+/* Returns cuda device number associated with communicator. */
+ncclResult_t ncclCommCuDevice(const ncclComm_t comm, int* device);
+ncclResult_t pncclCommCuDevice(const ncclComm_t comm, int* device);
+
+/* Returns user-ordered "rank" assocaiated with communicator. */
+ncclResult_t  ncclCommUserRank(const ncclComm_t comm, int* rank);
+ncclResult_t pncclCommUserRank(const ncclComm_t comm, int* rank);
+
+/* Reduction opperation selector */
+typedef enum { ncclSum        = 0,
+               ncclProd       = 1,
+               ncclMax        = 2,
+               ncclMin        = 3,
+               nccl_NUM_OPS   = 4 } ncclRedOp_t;
+
+/* Data types */
+typedef enum { ncclChar       = 0,
+               ncclInt        = 1,
+#ifdef CUDA_HAS_HALF
+               ncclHalf       = 2,
+#endif
+               ncclFloat      = 3,
+               ncclDouble     = 4,
+               ncclInt64      = 5,
+               ncclUint64     = 6,
+               nccl_NUM_TYPES = 7 } ncclDataType_t;
+
+/* Reduces data arrays of length count in sendbuff into recvbuf using op operation.
+ * recvbuf may be NULL on all calls except for root device.
+ * On the root device, sendbuff and recvbuff are assumed to reside on
+ * the same device.
+ * Must be called separately for each communicator in communicator clique.
+*/
+ncclResult_t  ncclReduce(const void* sendbuff, void* recvbuf, int count, ncclDataType_t datatype,
+    ncclRedOp_t op, int root, ncclComm_t comm, cudaStream_t stream);
+ncclResult_t pncclReduce(const void* sendbuff, void* recvbuf, int count, ncclDataType_t datatype,
+    ncclRedOp_t op, int root, ncclComm_t comm, cudaStream_t stream);
+
+/* Reduces data arrays of length count in sendbuff using op operation, and leaves
+ * identical copies of result on each GPUs recvbuff.
+ * Sendbuff and recvbuff are assumed to reside on the same device.
+ * Must be called separately for each communicator in communicator clique. */
+ncclResult_t  ncclAllReduce(const void* sendbuff, void* recvbuff, int count,
+    ncclDataType_t datatype, ncclRedOp_t op, ncclComm_t comm, cudaStream_t stream);
+ncclResult_t pncclAllReduce(const void* sendbuff, void* recvbuff, int count,
+    ncclDataType_t datatype, ncclRedOp_t op, ncclComm_t comm, cudaStream_t stream);
+
+/* Reduces data in sendbuff using op operation and leaves reduced result scattered
+ * over the devices so that recvbuff on the i-th GPU will contain the i-th block of
+ * the result. Sendbuff and recvbuff are assumed to reside on same device. Assumes
+ * sendbuff has size at least ndev*recvcount elements, where ndev is number of
+ * communicators in communicator clique
+ * Must be called separately for each communicator in communicator clique.*/
+ncclResult_t  ncclReduceScatter(const void* sendbuff, void* recvbuff,
+    int recvcount, ncclDataType_t datatype, ncclRedOp_t op, ncclComm_t comm,
+    cudaStream_t stream);
+ncclResult_t pncclReduceScatter(const void* sendbuff, void* recvbuff,
+    int recvcount, ncclDataType_t datatype, ncclRedOp_t op, ncclComm_t comm,
+    cudaStream_t stream);
+
+/* Copies count values from root to all other devices.
+ * Root specifies the source device in user-order
+ * (see ncclCommInit).
+ * Must be called separately for each communicator in communicator clique. */
+ncclResult_t  ncclBcast(void* buff, int count, ncclDataType_t datatype, int root,
+    ncclComm_t comm, cudaStream_t stream);
+ncclResult_t pncclBcast(void* buff, int count, ncclDataType_t datatype, int root,
+    ncclComm_t comm, cudaStream_t stream);
+
+
+/* Each device gathers count values from other GPUs.
+ * Result is ordered by comm's logical device order.
+ * Assumes recvbuff has size at least ndev*count, where ndev is number of communicators
+ * in communicator clique.
+ * Sendbuff and recvbuff are assumed to reside on same device.
+ * Must be called separately for each communicator in communicator clique. */
+ncclResult_t  ncclAllGather(const void* sendbuff, int count, ncclDataType_t datatype,
+    void* recvbuff, ncclComm_t comm, cudaStream_t stream);
+ncclResult_t pncclAllGather(const void* sendbuff, int count, ncclDataType_t datatype,
+    void* recvbuff, ncclComm_t comm, cudaStream_t stream);
+
+
+/* The following collective operations are not implemented yet */
+///* Gather count values from each device to recvbuff.
+// * Result is ordered by comm's logical device order.
+// * recvbuff may be NULL for all calls except for root device.
+// * On the root device, sendbuff and recvbuff are assumed to reside on the same device.
+// * Must be called separately for each communicator in communicator clique. */
+// * All GPUs, including root, perform copies into recvbuff.
+//ncclResult_t  ncclGather(const void* sendbuff, int count, ncclDataType_t datatype,
+//    void* recvbuff, int root, ncclComm_t comm, cudaStream_t stream);
+//ncclResult_t pncclGather(const void* sendbuff, int count, ncclDataType_t datatype,
+//                        void* recvbuff, int root, ncclComm_t comm, cudaStream_t stream);
+
+///* Root device scatters count values to each devices.
+// * sendbuff may be NULL on all devices except a single root
+// * device where it is assumed to have size at least nGPUs*count.
+// * recvbuff allocated on each gpu, including root, size=count.
+// * Result is ordered by comm's logical device order.
+// * Called separately for each device in the ncclComm. */
+//ncclResult_t  ncclScatter(void* sendbuff, ncclDataType_t datatype, void* recvbuff,
+//    int count, int root, ncclComm_t comm, cudaStream_t stream);
+//ncclResult_t pncclScatter(void* sendbuff, ncclDataType_t datatype, void* recvbuff,
+//    int count, int root, ncclComm_t comm, cudaStream_t stream);
+//
+///* All GPUs scatter blocks of count elements to other devices.
+// * Must be called separately for each device in the ncclComm.
+// * sendbuff and recvbuff assumed to reside on same device and
+// * have size at least nGPUs*count.
+// * Called separately for each device in the ncclComm. */
+//ncclResult_t  ncclAllToAll(void* sendbuff, int count, ncclDataType_t datatype,
+//    void* recvbuff, ncclComm_t comm, cudaStream_t stream);
+//ncclResult_t pncclAllToAll(void* sendbuff, int count, ncclDataType_t datatype,
+//    void* recvbuff, ncclComm_t comm, cudaStream_t stream);
+
+#ifdef __cplusplus
+} // end extern "C"
+#endif
+
+#endif // end include guard
+

